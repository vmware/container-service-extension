#cloud-config
write_files:
- path: /root/kubeadm-defaults.conf
  owner: root
  content: |
    ---
    apiVersion: kubeadm.k8s.io/v1beta2
    kind: InitConfiguration
    bootstrapTokens:
    - groups:
      - system:bootstrappers:kubeadm:default-node-token
      ttl: 0s
      usages:
      - signing
      - authentication
    nodeRegistration:
      criSocket: /run/containerd/containerd.sock
      kubeletExtraArgs:
        cloud-provider: external
    ---
    apiVersion: kubeadm.k8s.io/v1beta2
    kind: ClusterConfiguration
    controlPlaneEndpoint: "{control_plane_endpoint}"
    dns:
      type: CoreDNS
      imageRepository: projects.registry.vmware.com/tkg
      imageTag: __COREDNS_VERSION_SED_ME__
    etcd:
      local:
        imageRepository: projects.registry.vmware.com/tkg
        imageTag: __ETCD_VERSION_SED_ME__
    networking:
      serviceSubnet: {service_cidr}
      podSubnet: {pod_cidr}
    imageRepository: projects.registry.vmware.com/tkg
    kubernetesVersion: __KUBERNETES_VERSION_SED_ME__
    ---
- path: /root/vcloud-basic-auth.yaml
  owner: root
  content: |
    ---
    apiVersion: v1
    data:
      password: ""
      username: ""
      refreshToken: {base64_encoded_refresh_token}
    kind: Secret
    metadata:
      name: vcloud-basic-auth
      namespace: kube-system
    ---
- path: /root/default-storage-class.yaml
  owner: root
  content: |
    ---
    kind: StorageClass
    apiVersion: storage.k8s.io/v1
    metadata:
      annotations:
        storageclass.kubernetes.io/is-default-class: "true"
      name: {default_storage_class_name}
    provisioner: named-disk.csi.cloud-director.vmware.com
    reclaimPolicy: {storage_class_reclaim_policy}
    parameters:
      storageProfile: {vcd_storage_profile_name}
      filesystem: {storage_class_filesystem_type}
    ---
- path: /root/control_plane.sh
  owner: root
  content: |
    #!/usr/bin/env bash

    catch() {{
      retval=$?
       error_message="$(date) $(caller): $BASH_COMMAND"
       echo "$error_message" &>> /var/log/cse/customization/error.log
       vmtoolsd --cmd "info-set guestinfo.post_customization_script_execution_failure_reason $error_message"
       vmtoolsd --cmd "info-set guestinfo.post_customization_script_execution_status $retval"
    }}

    mkdir -p /var/log/cse/customization

    trap 'catch $? $LINENO' ERR

    set -ex

    echo "$(date) Post Customization script execution in progress" &>> /var/log/cse/customization/status.log

    kubeadm_config_path=/root/kubeadm-defaults.conf

    vcloud_basic_auth_path=/root/vcloud-basic-auth.yaml
    vcloud_configmap_path=/root/vcloud-configmap.yaml
    vcloud_ccm_path=/root/cloud-director-ccm.yaml

    vcloud_csi_configmap_path=/root/vcloud-csi-configmap.yaml
    csi_driver_path=/root/csi-driver.yaml
    csi_controller_path=/root/csi-controller.yaml
    csi_node_path=/root/csi-node.yaml
    kapp_controller_path=/root/kapp-controller.yaml

    vmtoolsd --cmd "info-set guestinfo.postcustomization.networkconfiguration.status in_progress"
      echo 'net.ipv6.conf.all.disable_ipv6 = 1' >> /etc/sysctl.conf
      echo 'net.ipv6.conf.default.disable_ipv6 = 1' >> /etc/sysctl.conf
      echo 'net.ipv6.conf.lo.disable_ipv6 = 1' >> /etc/sysctl.conf
      sudo sysctl -p

      # also remove ipv6 localhost entry from /etc/hosts
      sed -i 's/::1/127.0.0.1/g' /etc/hosts || true
    vmtoolsd --cmd "info-set guestinfo.postcustomization.networkconfiguration.status successful"


    vmtoolsd --cmd "info-set guestinfo.postcustomization.store.sshkey.status in_progress"
      ssh_key="{ssh_key}"
      if [[ ! -z "$ssh_key" ]];
      then
        mkdir -p /root/.ssh
        echo $ssh_key >> /root/.ssh/authorized_keys
        chmod -R go-rwx /root/.ssh
      fi
    vmtoolsd --cmd "info-set guestinfo.postcustomization.store.sshkey.status successful"

    vmtoolsd --cmd "info-set guestinfo.postcustomization.proxy.setting.status in_progress"
    export HTTP_PROXY="{http_proxy}"
    export HTTPS_PROXY="{https_proxy}"
    export http_proxy="{http_proxy}"
    export https_proxy="{https_proxy}"
    export NO_PROXY="{no_proxy}"
    export no_proxy="{no_proxy}"

    mkdir -p /etc/systemd/system/containerd.service.d
    cat <<END > /etc/systemd/system/containerd.service.d/http-proxy.conf
    [Service]
    Environment="HTTP_PROXY={http_proxy}"
    Environment="HTTPS_PROXY={https_proxy}"
    Environment="http_proxy={http_proxy}"
    Environment="https_proxy={https_proxy}"
    Environment="no_proxy={no_proxy}"
    Environment="NO_PROXY={no_proxy}"
    END
    systemctl daemon-reload
    systemctl restart containerd
    vmtoolsd --cmd "info-set guestinfo.postcustomization.proxy.setting.status successful"

    # openbracket(all caps) will be replaced by the open bracket and closebracket (all caps)
    # will be replaced by an open bracket.
    # This convention is needed so that python's template format function does not view the bash
    # $\openbracket/VAR/\closebracket as a format variable that will be replaced by the python format function.
    antrea_version="{antrea_version}"
    kapp_controller_version=""
    metrics_server_version=""
    metrics_server_version_valid=true
    vmtoolsd --cmd "info-set guestinfo.postcustomization.tkr.get_versions.status in_progress"
      tkr_bom_dir=/tmp/tkr_bom
      bom_path=$tkr_bom_dir/bom
      mkdir -p $bom_path
      components_path=$bom_path/components.yaml
      imgpkg_path=$tkr_bom_dir/imgpkg
      yq_path=$tkr_bom_dir/yq
      default_antrea_version="0.11.3"

      xml_version_property=$(vmtoolsd --cmd "info-get guestinfo.ovfenv" | grep "oe:key=\"VERSION\"")
      init_k8s_version=$(echo $xml_version_property | sed 's/.*oe:value=\"//; s/\(.*\)-.*/\1/')
      k8s_version=$(echo $init_k8s_version | tr -s "+" "_")

      # download imgpkg, which is needed for getting the components yaml file
      wget -nv github.com/vmware-tanzu/carvel-imgpkg/releases/download/v0.24.0/imgpkg-linux-amd64 -O $imgpkg_path
      chmod +x $imgpkg_path

      # We need to loop through the `X` value in `tkg.X` because of some TKR unexpected design.
      # We increment `X`, looking for a valid tkr bom version.
      no_tkr_found=false
      until $imgpkg_path pull -i projects.registry.vmware.com/tkg/tkr-bom:$OPENBRACKETk8s_versionCLOSEBRACKET -o $bom_path
      do
        tkg_version=$(echo $OPENBRACKETk8s_version//*.CLOSEBRACKET)
        tkg_version=$((tkg_version+1))
        k8s_version=$(echo $k8s_version | sed "s/.$/"$tkg_version"/")
        if [[ $tkg_version -gt 10 ]]; then
          no_tkr_found=true
          break
        fi
      done
      # This version is written into the extra config for worker nodes to use for tanzu-cli.
      # Note: this approach can be done in R1 clusters since there are no rolling upgrade.
      # However, this approach should not be done in capvcd clusters due to rolling upgrades.
      vmtoolsd --cmd "info-set guestinfo.postcustomization.tkr.get_versions.k8s $k8s_version"

      mv $bom_path/*.yaml $components_path
      # download yq for yaml parsing
      wget https://github.com/mikefarah/yq/releases/download/v4.2.0/yq_linux_amd64 -O $yq_path
      chmod +x $yq_path

      # handle getting antrea version
      if [[ -z "$antrea_version" ]]; then
        if [[ "$no_tkr_found" = true ]] ; then
          echo "no tkr bom found, will use default component versions"  &>> /var/log/cse/customization/status.log
          antrea_version=$default_antrea_version
        else
          # will get antrea version from tkr file
          antrea_version=$($yq_path e ".components.antrea[0].version" $components_path | sed 's/+.*//')
          if [[ -z "$antrea_version" ]] || [[ "$antrea_version" = "null" ]] || [[ "$antrea_version" = "false" ]]; then
            antrea_version=$default_antrea_version
          else
            antrea_version=$(echo $antrea_version | sed "s/v//") # remove leading `v`, which will be added later
          fi
        fi
      fi

      # get kapp-controller, which will be installed on the worker node
      # This version is retrieved here since the antrea version is already retrieved and installed
      # on the control plane node, so this avoids retrieving another core package version on the worker node
      kapp_controller_version=$($yq_path e ".components.kapp-controller[0].version" $components_path | sed 's/v//')

      # store tkr versions in extra config
      vmtoolsd --cmd "info-set guestinfo.postcustomization.tkr.get_versions.kapp_controller $kapp_controller_version"

      # cleanup components downloads
      rm -rf $tkr_bom_dir
    vmtoolsd --cmd "info-set guestinfo.postcustomization.tkr.get_versions.status successful"

    vmtoolsd --cmd "info-set guestinfo.postcustomization.kubeinit.status in_progress"
      # tag images
      coredns_image_version=""
      etcd_image_version=""
      kubernetes_version=""
      for image in "coredns" "etcd" "kube-proxy" "kube-apiserver" "kube-controller-manager" "kube-scheduler"
      do
        image_ref=$(ctr -n=k8s.io image list | cut -d" " -f1 | grep $image)
        ref_path=$(echo $image_ref | sed 's/:.*//')
        new_tag_version=$(echo $image_ref | sed 's/.*://' | sed 's/_/-/')
        ctr -n=k8s.io image tag $image_ref $ref_path:$new_tag_version

        # save image tags for later
        if [[ "$image" = "coredns" ]]; then
          sed -i "s/__COREDNS_VERSION_SED_ME__/$new_tag_version/g" $kubeadm_config_path
        elif [[ "$image" = "etcd" ]]; then
          sed -i "s/__ETCD_VERSION_SED_ME__/$new_tag_version/g" $kubeadm_config_path
        elif [[ "$image" = "kube-proxy" ]]; then # selecting other kube-* images would work too
          sed -i "s/__KUBERNETES_VERSION_SED_ME__/$new_tag_version/g" $kubeadm_config_path
        fi
      done

      kubeadm init --config $kubeadm_config_path --v=10 &> /root/kubeadm-init.out
      export KUBECONFIG=/etc/kubernetes/admin.conf
    vmtoolsd --cmd "info-set guestinfo.kubeconfig $(cat /etc/kubernetes/admin.conf | base64 | tr -d '\n')"
    vmtoolsd --cmd "info-set guestinfo.postcustomization.kubeinit.status successful"

    vmtoolsd --cmd "info-set guestinfo.postcustomization.kubectl.cni.install.status in_progress"
      antrea_path=/root/antrea-$OPENBRACKETantrea_versionCLOSEBRACKET.yaml
      wget -O $antrea_path https://github.com/vmware-tanzu/antrea/releases/download/v$OPENBRACKETantrea_versionCLOSEBRACKET/antrea.yml
      # This does not need to be done from v0.12.0 onwards inclusive
      sed -i "s/image: antrea\/antrea-ubuntu:v$OPENBRACKETantrea_versionCLOSEBRACKET/image: projects.registry.vmware.com\/antrea\/antrea-ubuntu:v$OPENBRACKETantrea_versionCLOSEBRACKET/g" $antrea_path
      kubectl apply -f $antrea_path
      vmtoolsd --cmd "info-set guestinfo.postcustomization.core_packages.antrea_version $antrea_version"
    vmtoolsd --cmd "info-set guestinfo.postcustomization.kubectl.cni.install.status successful"


    vmtoolsd --cmd "info-set guestinfo.postcustomization.kubectl.cpi.install.status in_progress"
      kubectl apply -f $vcloud_basic_auth_path

      wget -O $vcloud_configmap_path https://raw.githubusercontent.com/vmware/cloud-provider-for-cloud-director/{cpi_version}/manifests/vcloud-configmap.yaml
      sed -i 's/VCD_HOST/"{vcd_host}"/' $vcloud_configmap_path
      sed -i 's/ORG/"{org}"/' $vcloud_configmap_path
      sed -i 's/OVDC/"{vdc}"/' $vcloud_configmap_path
      sed -i 's/NETWORK/"{network_name}"/' $vcloud_configmap_path
      sed -i 's/VIP_SUBNET_CIDR/"{vip_subnet_cidr}"/' $vcloud_configmap_path
      sed -i 's/VAPP/{cluster_name}/' $vcloud_configmap_path
      sed -i 's/CLUSTER_ID/{cluster_id}/' $vcloud_configmap_path
      kubectl apply -f $vcloud_configmap_path

      wget -O $vcloud_ccm_path https://raw.githubusercontent.com/vmware/cloud-provider-for-cloud-director/{cpi_version}/manifests/cloud-director-ccm.yaml
      kubectl apply -f $vcloud_ccm_path
    vmtoolsd --cmd "info-set guestinfo.postcustomization.kubectl.cpi.install.status successful"


    vmtoolsd --cmd "info-set guestinfo.postcustomization.kubectl.csi.install.status in_progress"
      wget -O $vcloud_csi_configmap_path https://raw.githubusercontent.com/vmware/cloud-director-named-disk-csi-driver/{csi_version}/manifests/vcloud-csi-config.yaml
      sed -i 's/VCD_HOST/"{vcd_host}"/' $vcloud_csi_configmap_path
      sed -i 's/ORG/"{org}"/' $vcloud_csi_configmap_path
      sed -i 's/OVDC/"{vdc}"/' $vcloud_csi_configmap_path
      sed -i 's/VAPP/{cluster_name}/' $vcloud_csi_configmap_path
      sed -i 's/CLUSTER_ID/"{cluster_id}"/' $vcloud_csi_configmap_path
      kubectl apply -f $vcloud_csi_configmap_path

      wget -O $csi_driver_path https://raw.githubusercontent.com/vmware/cloud-director-named-disk-csi-driver/{csi_version}/manifests/csi-driver.yaml
      wget -O $csi_controller_path https://raw.githubusercontent.com/vmware/cloud-director-named-disk-csi-driver/{csi_version}/manifests/csi-controller.yaml
      wget -O $csi_node_path https://raw.githubusercontent.com/vmware/cloud-director-named-disk-csi-driver/{csi_version}/manifests/csi-node.yaml
      kubectl apply -f $csi_driver_path
      kubectl apply -f $csi_controller_path
      kubectl apply -f $csi_node_path
    vmtoolsd --cmd "info-set guestinfo.postcustomization.kubectl.csi.install.status successful"

    vmtoolsd --cmd "info-set guestinfo.postcustomization.kubectl.default_storage_class.status in_progress"
      create_default_storage_class={create_default_storage_class}
      if [[ "$create_default_storage_class" = true ]] ; then
        kubectl apply -f /root/default-storage-class.yaml
      fi
    vmtoolsd --cmd "info-set guestinfo.postcustomization.kubectl.default_storage_class.status successful"


    vmtoolsd --cmd "info-set guestinfo.postcustomization.kubeadm.token.generate.status in_progress"
      kubeadm_join_info=$(kubeadm token create --print-join-command --ttl 0 2> /dev/null)
      vmtoolsd --cmd "info-set guestinfo.postcustomization.kubeadm.token.info $kubeadm_join_info"
    vmtoolsd --cmd "info-set guestinfo.postcustomization.kubeadm.token.generate.status successful"

    echo "$(date) post customization script execution completed" &>> /var/log/cse/customization/status.log
    exit 0
runcmd:
- '[ ! -f /root/kubeadm-defaults.conf ] && cloud-init clean && sudo reboot'
- '[ ! -f /root/vcloud-basic-auth.yaml ] && cloud-init clean && sudo reboot'
- '[ ! -f /root/control_plane.sh ] && cloud-init clean && sudo reboot'
- bash /root/control_plane.sh
timezone: UTC
disable_root: false
preserve_hostname: false
hostname: {vm_host_name}
final_message: "The system is ready after $UPTIME seconds"
